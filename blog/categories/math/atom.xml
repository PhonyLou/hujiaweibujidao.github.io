<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: math | Hujiawei Bujidao]]></title>
  <link href="http://hujiaweibujidao.github.io/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://hujiaweibujidao.github.io/"/>
  <updated>2014-05-20T17:13:23+08:00</updated>
  <id>http://hujiaweibujidao.github.io/</id>
  <author>
    <name><![CDATA[hujiawei]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[SS 4-Hypothesis Testing]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-4/"/>
    <updated>2014-05-19T23:00:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-4</id>
    <content type="html"><![CDATA[<p><strong><center>统计学那些事 Things of Statistics</center></strong>
<strong><center>逸夫图书馆, 2014/5/19</center></strong></p>

<hr />

<h4 id="center-center"><center>第四部分 假设检验</center></h4>

<p>1.假设：一般假设就是一个“猜想”，它表述问题的一般陈述。假设检验是用于样本，然后才将结论一般化推广到总体中。</p>

<p>2.零假设(null hypothesis=$H_{0}$，或叫原假设)：它一般表示“正在研究的两个变量无关或者没有差异”这样的命题。例如，三年级学生的记忆力考试成绩与四年级学生记忆力考试成绩之间没有差异。</p>

<p><strong>(1)零假设是研究的起点，因为在没有信息的情况下，零假设就被看作是可以接受的真实状态。在这种假设下，我们认为观测到的效应是由偶然因素造成的。</strong></p>

<p><strong>(2)零假设也是研究的基准，也就是说在零假设成立的情况下，计算统计量，然后进行假设检验。这就类似反证法的思想。</strong></p>

<p>3.研究假设(research hypothesis=alternate hypothesis，或叫备择假设)：与零假设相对立的，认为变量之间有关系的假设。</p>

<p>研究假设分为有方向和无方向两种研究假设。无方向研究假设命题例子：三年级学生的记忆力考试成绩与四年级学生记忆力考试成绩之间有差异。有方向研究假设命题例子：三年级学生的记忆力考试成绩低于四年级学生记忆力考试成绩。</p>

<p>讨论有无方向的另一种形式是讨论单尾检验(one-tailed test)和双尾检验(two-tailed test)。</p>

<p><strong>零假设与研究假设的区别：</strong></p>

<p><strong>(1)零假设表示两个变量没有差异或者没有关系，研究假设表示它们有关系或者有差异；</strong></p>

<p><strong>(2)零假设对应的是总体，而研究假设对应的是样本。我们是从总体中取出一部分样本进行检验，将得到的结论推广到总体中。</strong></p>

<p><strong>(3)因为总体不能直接检验(不现实，不经济或者不可能)，所以零假设只能间接检验，研究假设则可以直接检验。</strong></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SS 3-Continuous Distribution]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2-2/"/>
    <updated>2014-05-19T20:40:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2-2</id>
    <content type="html"><![CDATA[<p><strong><center>统计学那些事 Things of Statistics</center></strong>
<strong><center>逸夫图书馆, 2014/5/19</center></strong></p>

<hr />

<h4 id="center-center"><center>第三部分 分布之多维随机变量及其分布</center></h4>

<p>很多情况下我们遇到的都是多维的随机变量，比如，对一个地区的儿童进行抽样统计，观察他们的身高H和体重W，样本空间S就是该地区的儿童，身高H和体重W都是定义在S上的随机变量，这里向量(H,W)就构成了二维随机向量(或者二维随机变量)，前面两节讨论的分布都是一维随机变量的分布。二维随机变量(X,Y)的性质不仅和X及Y的性质有关，还和它们的相关性有关，也就是前面提到的相关系数！</p>

<p>1.二维随机变量(X,Y)的分布函数(或者叫联合分布函数)</p>

<script type="math/tex; mode=display">F(x,y)=P(X \le x \cap Y \le y)=P(X \le x, Y \le y)</script>

<p>如果将二维随机变量看作是平面上随机点的坐标的话，那么分布函数F(x,y)在点(x,y)处的函数值就是以点(x,y)为顶点而位于该点左下方的无穷矩形域内的概率。</p>

<p>分布函数的性质：</p>

<p>二维离散型随机变量(X,Y) [也就是只有有限对可取的值]的分布律(或者联合分布律)：</p>

<p><script type="math/tex">P(X=x_{i},Y=y_{j})=p_{ij},i,j=1,2,...</script>且有<script type="math/tex">\Sigma_{i=0}^{\infty}\Sigma_{j=0}^{\infty}p_{ij}=1</script></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SS 3-Continuous Distribution]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2-1/"/>
    <updated>2014-05-19T20:30:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2-1</id>
    <content type="html"><![CDATA[<p><strong><center>统计学那些事 Things of Statistics</center></strong>
<strong><center>逸夫图书馆, 2014/5/19</center></strong></p>

<hr />

<h4 id="center-center"><center>第三部分 分布之连续型随机变量分布</center></h4>

<p>连续型随机变量，概率密度及它的性质</p>

<p>函数f(x)是概率密度函数，函数F(x)是分布函数，两者都是连续函数。</p>

<script type="math/tex; mode=display">F(x)=\int_{-\infty}^{x}f(t)dt</script>

<p><a href="http://hujiaweibujidao.github.io/images/math/cont0.png">查看定义1</a>
<a href="http://hujiaweibujidao.github.io/images/math/cont1.png">查看定义2</a></p>

<p>关于连续型随机变量X对于任意一个指定实数值k的概率都是0，即$p(X=k)=0$的解释</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont2.png" alt="image" /></p>

<p>(1)均匀分布(uniform distribution)</p>

<p>连续型随机变量X在区间(a,b)均匀分布，它的期望是$E=\frac{a+b}{2}$，也就是期望就是区间(a,b)的中点，它的方差是$D=\frac{(b-a)^{2}}{12}$，用$D=E(X^{2})-E^{2}$去证明方便些。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_uniform.png" alt="image" /></p>

<p>(2)指数分布(exponential distribution)</p>

<p>指数分布有一个参数$\theta$，它的期望就是$\theta$，方差是$\theta^{2}$，而且它具有<strong>无记忆性</strong>。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_exp1.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/cont_exp2.png" alt="image" /></p>

<p>证明它的期望是$\theta$，方差是$\theta^{2}$:</p>

<script type="math/tex; mode=display">E(X)=\int_{-\infty}^{\infty}xf(x)dx = \int_{0}^{\infty}x \frac{1}{\theta} e^{- \frac{x}{\theta}}dx = \int_{0}^{\infty}xd(-e^{- \frac{x}{\theta}}) = [-x e^{- \frac{x}{\theta}}]_{0}^{\infty} +  \int_{0}^{\infty} e^{- \frac{x}{\theta}}dx = \theta</script>

<script type="math/tex; mode=display">E(X^{2})=\int_{-\infty}^{\infty}x^{2}f(x)dx = \int_{0}^{\infty}x^{2} \frac{1}{\theta} e^{- \frac{x}{\theta}}dx = \int_{0}^{\infty}x^{2}d(-e^{- \frac{x}{\theta}}) = [-x^{2} e^{- \frac{x}{\theta}}]_{0}^{\infty} +  \int_{0}^{\infty} 2x e^{- \frac{x}{\theta}}dx = 2\theta^{2}</script>

<script type="math/tex; mode=display">D(X)=E(X^{2})-[E(X)]^{2}=\theta^{2}</script>

<p>《统计思维》对指数分布的解释：举例来说，<strong>观察一系列事件之间的间隔时间，若事件在每个时间点发生的概率相同，那么间隔时间的分布就近似指数分布</strong>(也就是前面的无记忆性)。</p>

<p>指数分布的CDF如下，此时$\lambda=\frac{1}{\theta}$</p>

<script type="math/tex; mode=display">
CDF(x)=1-e^{-\lambda x}
</script>

<p>参数$\lambda$决定了指数分布的形状，通常，指数分布的均值是$\frac{1}{\lambda}$，中位数是$\frac{log(2)}{\lambda}$。下图为$\lambda=2$的指数分布图：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/edcdf.png" alt="image" /></p>

<p>如何判断一个分布是否是指数分布呢？一种办法是画出取对数之后的互补累积分布函数(CCDF=Complementary CDF=1-CDF(x))，CCDF是一条斜率为$-\lambda$的直线，原因如下：</p>

<script type="math/tex; mode=display">
y=CCDF(x)=1-CDF(x)=e^{-\lambda x} \quad => \quad log(y)=-\lambda x
</script>

<p>(3)正态分布(normal distribution)</p>

<p>正态分布，又叫高斯分布，是最常用的分布。其中$x=\mu$是函数f(x)的驻点，$x=\mu+\sigma,  x=\mu-\sigma$是函数f(x)的拐点，这个可能不太好计算，可用下面的Matlab代码进行验证。</p>

<p><code>matlab
clear,clc;
syms x;
syms u;
syms r;
syms p;
f=1 / (sqrt(2*p)*r) * exp(-1 * (x-u)^2 / (2*r^2) );
pretty(f)
f1=diff(f, 1) %一阶导数
pretty(f1)
f2=diff(f, 2) %二阶导数
pretty(f2)
solve(f1) %u
solve(f2) %u+r u-r
</code></p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_normal1.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/cont_normal2.png" alt="image" /></p>

<p>关于标准正态分布，即参数为$\mu=0, \sigma=1$的正态分布，它的分布函数值已经制作成表格，可以方便进行查看，其他非标准正态分布可以通过一个线性变换转换成标准正态分布。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_normal4.png" alt="image" /></p>

<p>在证明其概率密度总和为1时利用了一个重要的积分$\int_{-\infty}^{\infty} e^{\frac{t^{2}}{2}} dt = \sqrt{2 \pi}$，它的证明可以转换成二重积分然后通过极坐标计算出来。完整详细的证明可以参考下面：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/inte1.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/inte2.png" alt="image" /></p>

<p>《统计思维》对正态分布的解释：对于正态分布的CDF还没有一种准确的表达，最常用的一种形式是以误差函数(error function)来表示，它是一个特殊的函数，表示为erf(x)，在Matlab中内置了函数<code>erf</code>，对它的说明为erf函数是对参数为$\mu=0, \sigma=\frac{1}{2}$的正态分布的二重积分，有兴趣可以去计算一下，得到的结果如下：</p>

<script type="math/tex; mode=display">
CDF(x)=\frac{1}{2}[1+erf(\frac{x-\mu}{\sigma \sqrt{2}})] \quad erf(x)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-t^{2}}dt
</script>

<p>其中，参数$\mu$和$\sigma$分别决定了正态分布的均值和标准差。下图为$\mu=2.0$和$\sigma=0.5$的正态分布的CDF图：[呈现明显的S型]</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/ndcdf.png" alt="image" /></p>

<p>根据大数定理，当我们处理大样本数据集(超过30个数据)，并且重复地从总体中抽取样本时，得到的数值分布就接近正态分布曲线。正态分布以均值为中心完全对称。</p>

<p>关于正态分布有一个重要的结论，对任何数值分布来说(不论它的均值和标准差)，只要数值是正态分布，那么几乎100%的数值都分布在均值的-3到3个标准差之间。下面是正态曲线下数值的分布情况：</p>

<!--
![image](http://hujiaweibujidao.github.io/images/math/nd.png)
-->

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_normal3.png" alt="image" /></p>

<p>从中可以看出，<strong>在距离均值1个标准差之间大概有34%的数值分布，在1个标准差和2个标准差之间大概有13%的数值分布，在2个标准差和3个标准差之间大概有2.1%的数值分布。</strong></p>

<p>通过这个图我么可以得到一个经典的<strong>3$\sigma$法则</strong>，又叫<strong>68-95-99法则</strong></p>

<p><strong>在$(\mu - \sigma, \mu + \sigma)$之间大概有68.26%的数据分布，在$(\mu - 2\sigma, \mu + 2\sigma)$之间大概有95.44%的数据分布，在$(\mu - 3\sigma, \mu + 3\sigma)$之间大概有99.74%的数据分布。</strong></p>

<p>$\alpha$分位点的概念：对于标准正态分布X~N(0,1)，满足<script type="math/tex">P(X>Z_{\alpha})=\alpha</script>的<script type="math/tex">Z_{\alpha}</script>称为$\alpha$分位点，且有<script type="math/tex">Z_{-1\alpha}=Z_{\alpha}</script>。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cont_normal5.png" alt="image" /></p>

<p><a href="http://wikipedia.org/wiki/Log-normal_distribution">对数正态分布 on wiki</a>：如果一组数据取对数之后服从正态分布，那么我们就称其服从对数正态分布。对数正态分布的 CDF 跟正态分布一样, 只是用 logx 代替原来的 x:</p>

<script type="math/tex; mode=display">
CDF_{lognormal}(x) = CDF_{normal}(log x)
</script>

<p>对数正态分布的均值与标准差不再是是$\mu$和$\sigma$了。可以证明，成人体重的分布是近似对数正态的。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SS 3-Discrete Distribution]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2/"/>
    <updated>2014-05-19T20:00:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-2</id>
    <content type="html"><![CDATA[<p><strong><center>统计学那些事 Things of Statistics</center></strong>
<strong><center>逸夫图书馆, 2014/5/19</center></strong></p>

<hr />

<h4 id="center-center"><center>第三部分 分布之离散型随机变量分布</center></h4>

<p>1.概率质量函数PMF(Probability Mass Function)</p>

<p>数据集的数据值到它的概率的映射函数。直方图是各个值出现的频数，如果将频数除以样本总数，得到概率，归一化之后的直方图就是PMF。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/pmf.png" alt="image" /></p>

<p>2.累积分布函数CDF(Cumulative Distribution Function)</p>

<p>数据集的数据值到它在分布中概率的累积值的映射函数。PMF和CDF在国内的教材中并没有这样提过，但是在国外的很多统计书中都有，所以还是比较重要的，拿出来介绍下。</p>

<p>例如，CDF(0) = 0; CDF(1) = 0.2; CDF(2) = 0.6; CDF(3) = 0.8; CDF(4) = 0.8; CDF(5) = 1，它的CDF图为一个阶跃函数：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/cdf.png" alt="image" /></p>

<p>3.离散型随机变量及其分布律</p>

<p>分布律：离散型随机变量以概率1和一定的规律分布在一些离散值上</p>

<p><a href="http://hujiaweibujidao.github.io/images/math/disc0.png">查看定义</a></p>

<p>(1)0-1分布</p>

<p>随机变量X只有两个取值0和1(样本空间只有两个取值也行)，所以叫做0-1分布，它的分布律为
$P(X=0)=p, P(X=1)=q, (q=1-p)$，它的期望是$p$，方差是$p(1-p)$。</p>

<p><a href="http://hujiaweibujidao.github.io/images/math/disc_01.png">查看定义</a></p>

<p>(2)二项分布</p>

<p>二项分布的分布律为 $P(X=k)= {n \choose k} p^{k}q^{1-k}$，因为 $P(X=k)$ 刚好是 $(p+q)^{n}$ 的二项式系数，所以这个分布就叫二项分布。二项分布是从n重伯努利试验中得到的分布，所谓的伯努利试验就是指相互独立的试验，每次试验的结果要么成功要么失败(或者说某个事件要么发生要么不发生)。</p>

<p><a href="http://hujiaweibujidao.github.io/images/math/disc_binomial.png">查看定义1</a>
<a href="http://hujiaweibujidao.github.io/images/math/disc_binomial2.png">查看定义2</a></p>

<p>(3)泊松分布</p>

<p>泊松分布是一类很常用的分布，它的分布律是$P(X=k)=\frac{\lambda^{k} e^{-\lambda}}{k!},(k=0,1,2,…)$，其中有一个参数$\lambda$，参数$\lambda$的含义既是泊松分布的期望，又是它的方差，所以，只要参数$\lambda$或者期望或者方差确定了，泊松分布就确定了。泊松分布中经常需要用到的式子[$e^{\lambda}=\Sigma_{k=0}^{\infty}\frac{x^{k}}{k!}$]</p>

<p><a href="http://hujiaweibujidao.github.io/images/math/disc_pos.png">查看定义</a></p>

<p>证明它的期望是$\lambda$: </p>

<script type="math/tex; mode=display">E=\Sigma_{k=0}^{\infty}k \frac{\lambda^{k} e^{-\lambda}}{k!}=\lambda e^{-\lambda} \Sigma_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}=\lambda e^{-\lambda} e^{\lambda}=\lambda</script>

<p>证明它的方差是$\lambda$:</p>

<script type="math/tex; mode=display">D=E(X^{2})-E^{2}=E(X(X-1)-X)-E^{2}=\Sigma_{k=0}^{\infty}k(k-1) \frac{\lambda^{k} e^{-\lambda}}{k!}+\lambda-\lambda^{2}=\lambda e^{-\lambda} \Sigma_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}=\lambda^{2} e^{-\lambda} e^{\lambda}+\lambda-\lambda^{2}=\lambda</script>

<p><strong>泊松定理说明当n很大，p很小的时候，以n，p为参数的二项分布可以用参数$\lambda = np $的泊松分布进行近似！</strong>记住这个定理其实也可以方便我们记住泊松分布的分布律。</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/disc_pos2.png" alt="image" />
<img src="http://hujiaweibujidao.github.io/images/math/disc_pos3.png" alt="image" /></p>

<p>应用举例，记住后面的结论：</p>

<p><strong>当$n \ge 20,p \le 0.05$时用泊松分布近似二项分布的概率值近似效果颇佳。</strong></p>

<p><img src="http://hujiaweibujidao.github.io/images/math/disc_pos4.png" alt="image" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SS 2-Descriptive Statistics]]></title>
    <link href="http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-3/"/>
    <updated>2014-05-19T19:30:00+08:00</updated>
    <id>http://hujiaweibujidao.github.io/blog/2014/05/19/statistics-summary-3</id>
    <content type="html"><![CDATA[<p><strong><center>统计学那些事 Things of Statistics</center></strong>
<strong><center>逸夫图书馆, 2014/5/19</center></strong></p>

<hr />

<h4 id="center-center"><center>第二部分 描述性统计量</center></h4>

<p>1.描述数据的集中趋势：均值(mean)，中位数(median)，众数(mode)，加权平均数</p>

<p>百分位点(percentile points)：中位数(Q2)就是50百分位点，Q1为25百分位点(lower quartile)，Q3为75百分位点(upper quartile)，<strong>经常使用Q3-Q1=IQR(interquartile range，四分差或四分位数)来检查分布是否对称。</strong></p>

<!--
[如果要计算一组数中的某个百分位数，一般比较好的排序方法是选择排序；当然，如果是计算该组数的特殊的百分位数，例如中位数，有其他更好地方法能够在线性时间内得到，之后我对做一些相关问题的研究，暂且说明一下]
-->

<p>2.描述数据的变异性：极差(range)，标准差(standard deviation,简称s或者SD)，方差(deviation)</p>

<p>标准差的计算公式：
$$
s=\sqrt{\frac{\Sigma(X-\bar{X})^2}{n-1}}
$$</p>

<p><strong>s是总体标准差的无偏估计，如果根号内部分母改成了n则是有偏估计，详细证明参见：<a href="http://en.wikipedia.org/wiki/Bias_of_an_estimator">http://en.wikipedia.org/wiki/Bias_of_an_estimator</a>，《爱上统计学》作者对此的解释是统计学家们通常比较保守，保守的含义是，如果我们不得不出错，我们出错也是因为过高地估计了标准差(因为除以n-1使得标准差大于实际值)。</strong></p>

<p>如果想了解更加细致的内容可以看下这篇文章<a href="http://www.visiondummy.com/2014/03/divide-variance-n-1/">Why divide the sample variance by N-1?</a></p>

<p>标准差和方差的异同：<strong>它们都是用来反映数据集的数据的变异性或者离散度的度量，但是标准差以原有的计算单位存在，然而方差以平方单位存在，前者在实际中更加具有意义</strong>。例如，某高校的男生的平均身高是170cm，标准差是5cm，那么说明该校男生的身高与平均身高的差异大概就是上下5cm，换成方差来解释的话就不好陈述了。</p>

<p>使用有偏估计其实也可以，但是最好使用无偏估计，我记得Coursera上Machine Learning课中Andrew Ng曾经提到过，实际编码中其实还是使用有偏估计，因为它们在样本数据很大的时候其实结果没多大影响。</p>

<p>3.数据集的图形化显示：直方图，饼图，线图，柱形图，条形图，茎叶图等</p>

<p>数据分布的差异性描述：平均值，变异性，峰度(kurtosis)，偏度(skewness)</p>

<p>峰值可能有多个，比如双峰或者多峰等。偏度有一个计算公式，由Pearson发明的，他同时也是相关系数的发明者，偏度虽有正负之分，但是绝对值越大说明图形越偏。</p>

<script type="math/tex; mode=display">
SK=\frac{3(\bar{X}-M)}{s},\quad M=Median,\bar{X}=Mean,s=SD
</script>

<p>峰度图：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/kurtosis.png" alt="image" /></p>

<p>偏度图：</p>

<p><img src="http://hujiaweibujidao.github.io/images/math/skewness.png" alt="image" /></p>

<p>4.相关系数(correlation coefficient)是两个变量之间<strong>线性关系</strong>的数值型指标，取值范围是[-1,1]，大于0表示正相关，小于0表示负相关，可以用散点图来直接查看相关性。<strong>根据某些不成文的规则，一般高于0.6表示强相关，低于0.4表示弱相关，中间部分表示中度相关。</strong></p>

<p>[<strong>Pearson相关系数考察的变量的属性是连续的，例如年龄，体重等，如果是离散型变量那么应该使用点二列相关系数</strong>]</p>

<p>注意两点：</p>

<p>(1)<strong>相关系数反映的是只是线性关系！如果两个变量的相关系数为0，只能说明它们没有线性关系存在，但是可能存在其他的非线性关系！</strong></p>

<p>(2)<strong>相关性和因果关系无关！</strong>例如，冰淇淋的消费量和犯罪率是正相关的，但是两者不存在任何因果关系！</p>

<p>相关系数的计算：</p>

<script type="math/tex; mode=display">
r_{XY}=\frac{n\Sigma{XY}-\Sigma{X}\Sigma{Y}}{\sqrt{[n\Sigma{X^2}-(\Sigma{X})^2][n\Sigma{Y^2}-(\Sigma{Y})^2]}}
</script>

<p>决定系数：相关系数的平方，它表述一个变量的方差可以被另一个变量的方差来解释的百分比。</p>

]]></content>
  </entry>
  
</feed>
